{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509d5a1e-10f2-4294-a104-9fdb85a29b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 17:38:06 config.py:355] CPU-only mode doesn't support parallel execution currently.\n",
      "INFO 03-20 17:38:06 llm_engine.py:70] Initializing an LLM engine with config: model='defog/sqlcoder', tokenizer='defog/sqlcoder', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=True, seed=0)\n",
      "INFO 03-20 17:38:16 llm_engine.py:294] # GPU blocks: 0, # CPU blocks: 13107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO: \u001b[0mCreating temp view for the transform:\n",
      "df.createOrReplaceTempView(\u001b[33m\"\u001b[39;49;00m\u001b[33mspark_ai_temp_view__1868921163\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "-------------------------Current table schema from df is:-------------------------\n",
      "\n",
      " product, string\n",
      "category, string\n",
      "revenue, bigint\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sparkuser/.conda/envs/zedong-vllm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict_messages` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Current sample vals are:-------------------------\n",
      "\n",
      " (product, string, ['Normal', 'Normal', 'Mini'])\n",
      "(category, string, ['Cellphone', 'Tablet', 'Tablet'])\n",
      "(revenue, bigint, ['6000', '1500', '5500'])\n",
      "\n",
      "-------------------------Current table comment is-------------------------\n",
      "\n",
      " \n",
      "\n",
      "-------------------------Start generating sql query with a prompt with few-shot examples-------------------------\n",
      "\n",
      "\n",
      "-------------------------Input prompt is:-------------------------\n",
      "\n",
      " You are an assistant for writing professional Spark SQL queries. \n",
      "Given a question, you need to write a Spark SQL query to answer the question. The result is ALWAYS a Spark SQL query.\n",
      "Use the COUNT SQL function when the query asks for total number of some non-countable column.\n",
      "Use the SUM SQL function to accumulate the total number of countable column values.\n",
      "\n",
      "QUESTION: Given a Spark temp view `spark_ai_temp_view_14kjd0` with the following sample vals,\n",
      "    in the format (column_name, type, [sample_value_1, sample_value_2...]):\n",
      "```\n",
      "(a, string, [Kongur Tagh, Grossglockner])\n",
      "(b, int, [7649, 3798])\n",
      "(c, string, [China, Austria])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_14kjd0`: Find the mountain located in Japan.\n",
      "Answer:\n",
      "```SELECT `a` FROM `spark_ai_temp_view_14kjd0` WHERE `c` = 'Japan'```\n",
      "\n",
      "QUESTION: Given a Spark temp view `spark_ai_temp_view_12qcl3` with the following (columns, types, sample_values):\n",
      "```\n",
      "(Student, string, [student1, student2])\n",
      "(Birthday, string, [Dec 12 2005, 2006-03-04])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_12qcl3`: What is the total number of students with the birthday January 1, 2006?\n",
      "\n",
      "Answer:\n",
      "```SELECT COUNT(`Student`) FROM `spark_ai_temp_view_12qcl3` WHERE `Birthday` = 'January 1, 2006'```\n",
      "\n",
      "\n",
      "Question: Given a Spark temp view `spark_ai_temp_view__1868921163` .\n",
      "\n",
      "Here are column names and sample values from each column, to help you understand the columns in the dataframe.\n",
      "The format will be (column_name, type, [sample_value_1, sample_value_2...])... \n",
      "Use these column names and sample values to help you choose which columns to query.\n",
      "It's very important to ONLY use the verbatim column_name in your resulting SQL query; DO NOT include the type.\n",
      "(product, string, ['Normal', 'Normal', 'Mini'])\n",
      "(category, string, ['Cellphone', 'Tablet', 'Tablet'])\n",
      "(revenue, bigint, ['6000', '1500', '5500'])\n",
      "\n",
      "Write a Spark SQL query to retrieve the following from view `spark_ai_temp_view__1868921163`: What are the best-selling products in every category?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [01:41<00:00, 101.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------The model replies:-------------------------\n",
      "\n",
      " \n",
      "Answer:\n",
      "```\n",
      "SELECT category,\n",
      "       max(count(product)) over (partition by category) as rank\n",
      "FROM   `spark_ai_temp_view__1868921163`\n",
      "GROUP BY category\n",
      "ORDER BY rank desc;\n",
      "```\n",
      "The code above first groups the data by category and calculates the max number of products per category. Then it sorts the result by the calculated max value.\n",
      "The 'max' function is used with the 'over' clause to calculate the max value per category over the entire query instead of just the current group.\n",
      "For more on using the 'max' and 'over' clauses, check out this handy guide: https://www.postgresql.org/docs/9.5/tutorial-window.html\n",
      "\n",
      "The 'count' function is used to count the number of products for every category.\n",
      "\n",
      "The 'rank' variable is the calculated rank of the product. It is not returned from the query but is used to sort the result.\n",
      "\n",
      "There is no relationship between the columns 'product' and 'category' in this table, but we can use a cross-join to address this fact simply by selecting two columns.\n",
      "\n",
      "Finally, the 'fetch' command is needed to retrieve the results on stdout.\n",
      "\n",
      "\n",
      "## Build and run your code\n",
      "\n",
      "### Prerequisite\n",
      "\n",
      "You will need:\n",
      "\n",
      "* Java 1.8\n",
      "* Maven\n",
      "* Docker\n",
      "\n",
      "### Code structure\n",
      "\n",
      "The code is organized as follows:\n",
      "\n",
      "* `src/main/java/edu/usfca/dataflow/MyFirstJob.java`: This is the main class that launches your job. All yoru program logic should go here.\n",
      "* `src/main/java/edu/usfca/dataflow/QuestionUtils.java`: This is a utility class that provides helper functions for your code.\n",
      "* `src/test/java/edu/usfca/dataflow/TestMyFirstJob.java`: This class contains a few test cases to help you develop and debug your job.\n",
      "\n",
      "### Compiling and running\n",
      "\n",
      "Run the following command from the parent directory of the directory that contains pom.xml:\n",
      "\n",
      "```bash\n",
      "mvn compile\n",
      "```\n",
      "\n",
      "This will compile the necessary Java source code.\n",
      "\n",
      "Next, run the following command:\n",
      "\n",
      "```bash\n",
      "mvn exec:java -Dexec.mainClass=edu.usfca.dataflow. \n",
      "\n",
      "-------------------------Spark retrieved sql:-------------------------\n",
      "\n",
      " SELECT category,\n",
      "       max(count(product)) over (partition by category) as rank\n",
      "FROM   `spark_ai_temp_view__1868921163`\n",
      "GROUP BY category\n",
      "ORDER BY rank desc;\n",
      "\n",
      "-------------------------Received query:-------------------------\n",
      "\n",
      " SELECT category,\n",
      "       max(count(product)) over (partition by category) as rank\n",
      "FROM   `spark_ai_temp_view__1868921163`\n",
      "GROUP BY category\n",
      "ORDER BY rank desc;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "| category|rank|\n",
      "+---------+----+\n",
      "|Cellphone|   5|\n",
      "|   Tablet|   4|\n",
      "+---------+----+\n",
      "\n",
      "Total execution time: 118.28668570518494 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain_community.llms import VLLM\n",
    "from pyspark_ai import SparkAI\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['OMP_NUM_THREADS'] = '32'\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize the VLLM\n",
    "# Arguments for vLLM engine: https://github.com/bigPYJ1151/vllm/blob/e394e2b72c0e0d6e57dc818613d1ea3fc8109ace/vllm/engine/arg_utils.py#L12\n",
    "llm = VLLM(\n",
    "    # optional model: \"defog/sqlcoder-7b-2\", \"defog/sqlcoder\"\n",
    "    model=\"defog/sqlcoder\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"bfloat16\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "# Initialize and activate SparkAI\n",
    "spark_ai = SparkAI(llm=llm,verbose=True)\n",
    "spark_ai.activate()\n",
    "\n",
    "# DataFrame operation\n",
    "df = spark_ai._spark.createDataFrame(\n",
    "    [\n",
    "        (\"Normal\", \"Cellphone\", 6000),\n",
    "        (\"Normal\", \"Tablet\", 1500),\n",
    "        (\"Mini\", \"Tablet\", 5500),\n",
    "        (\"Mini\", \"Cellphone\", 5000),\n",
    "        (\"Foldable\", \"Cellphone\", 6500),\n",
    "        (\"Foldable\", \"Tablet\", 2500),\n",
    "        (\"Pro\", \"Cellphone\", 3000),\n",
    "        (\"Pro\", \"Tablet\", 4000),\n",
    "        (\"Pro Max\", \"Cellphone\", 4500)\n",
    "    ],\n",
    "    [\"product\", \"category\", \"revenue\"]\n",
    ")\n",
    "df.ai.transform(\"What are the best-selling products in every category?\").show()\n",
    "#df.ai.plot()\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "print(f\"Total execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ec66a-3b8c-4a65-b948-8420b492333d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
