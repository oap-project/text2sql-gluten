{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509d5a1e-10f2-4294-a104-9fdb85a29b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-28 22:32:38 config.py:468] float16 is not supported on CPU, casting to bfloat16.\n",
      "WARNING 03-28 22:32:38 config.py:480] Casting torch.float16 to torch.bfloat16.\n",
      "INFO 03-28 22:32:38 config.py:355] CPU-only mode doesn't support parallel execution currently.\n",
      "INFO 03-28 22:32:38 llm_engine.py:70] Initializing an LLM engine with config: model='defog/sqlcoder-7b-2', tokenizer='defog/sqlcoder-7b-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir='/mnt/DP_disk2/models/Huggingface/', load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=True, seed=0)\n",
      "INFO 03-28 22:32:39 llm_engine.py:294] # GPU blocks: 0, # CPU blocks: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init time: 5.173518896102905 seconds\n",
      "-------------------------Start get_transform_sql_query-------------------------\n",
      "\n",
      "\n",
      "\u001b[92mINFO: \u001b[0mCreating temp view for the transform:\n",
      "df.createOrReplaceTempView(\u001b[33m\"\u001b[39;49;00m\u001b[33mspark_ai_temp_view_1100037772\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "-------------------------Current table schema from df is:-------------------------\n",
      "\n",
      " product, string\n",
      "category, string\n",
      "revenue, bigint\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sparkuser/.conda/envs/zedong-vllm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict_messages` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Current sample vals are:-------------------------\n",
      "\n",
      " (product, string, ['Normal', 'Normal', 'Mini'])\n",
      "(category, string, ['Cellphone', 'Tablet', 'Tablet'])\n",
      "(revenue, bigint, ['6000', '1500', '5500'])\n",
      "\n",
      "-------------------------Current table comment is-------------------------\n",
      "\n",
      " \n",
      "\n",
      "-------------------------Start generating sql query with a prompt with few-shot examples-------------------------\n",
      "\n",
      "\n",
      "-------------------------Input prompt is:-------------------------\n",
      "\n",
      " You are an assistant for writing professional Spark SQL queries. \n",
      "Given a question, you need to write a Spark SQL query to answer the question.\n",
      "The rules that you should follow for answering question:\n",
      "1.The answer only consists of Spark SQL query. No explaination. No \n",
      "2.SQL statements should be  Spark SQL query.\n",
      "3.ONLY use the verbatim column_name in your resulting SQL query; DO NOT include the type.\n",
      "4.Use the COUNT SQL function when the query asks for total number of some non-countable column.\n",
      "5.Use the SUM SQL function to accumulate the total number of countable column values.\n",
      "\n",
      "QUESTION: Given a Spark temp view `spark_ai_temp_view_14kjd0` with the following sample vals,\n",
      "    in the format (column_name, type, [sample_value_1, sample_value_2...]):\n",
      "```\n",
      "(a, string, [Kongur Tagh, Grossglockner])\n",
      "(b, int, [7649, 3798])\n",
      "(c, string, [China, Austria])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_14kjd0`: Find the mountain located in Japan.\n",
      "Answer:\n",
      "```SELECT `a` FROM `spark_ai_temp_view_14kjd0` WHERE `c` = 'Japan'```\n",
      "\n",
      "QUESTION: Given a Spark temp view `spark_ai_temp_view_12qcl3` with the following (columns, types, sample_values):\n",
      "```\n",
      "(Student, string, [student1, student2])\n",
      "(Birthday, string, [Dec 12 2005, 2006-03-04])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_12qcl3`: What is the total number of students with the birthday January 1, 2006?\n",
      "\n",
      "Answer:\n",
      "```SELECT COUNT(`Student`) FROM `spark_ai_temp_view_12qcl3` WHERE `Birthday` = 'January 1, 2006'```\n",
      "\n",
      "\n",
      "Question: Given a Spark temp view `spark_ai_temp_view_1100037772`  with the following sample vals,\n",
      " in the format (column_name, type, [sample_value_1, sample_value_2...]):\n",
      "```\n",
      "(product, string, ['Normal', 'Normal', 'Mini'])\n",
      "(category, string, ['Cellphone', 'Tablet', 'Tablet'])\n",
      "(revenue, bigint, ['6000', '1500', '5500'])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_1100037772`: What is the best-selling product?\n",
      "Answer:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------The model replies:-------------------------\n",
      "\n",
      "  SELECT `product`, SUM(`revenue`) AS total_revenue FROM `spark_ai_temp_view_1100037772` GROUP BY `product` ORDER BY total_revenue DESC LIMIT 1 \n",
      "\n",
      "-------------------------Spark retrieved sql:-------------------------\n",
      "\n",
      "  SELECT `product`, SUM(`revenue`) AS total_revenue FROM `spark_ai_temp_view_1100037772` GROUP BY `product` ORDER BY total_revenue DESC LIMIT 1\n",
      "\n",
      "-------------------------End get_transform_sql_query-------------------------\n",
      "\n",
      " get_transform_sql_query_time: 6.660587787628174 seconds\n",
      "\n",
      "-------------------------Received query:-------------------------\n",
      "\n",
      "  SELECT `product`, SUM(`revenue`) AS total_revenue FROM `spark_ai_temp_view_1100037772` GROUP BY `product` ORDER BY total_revenue DESC LIMIT 1\n",
      "\n",
      "+-------+-------------+\n",
      "|product|total_revenue|\n",
      "+-------+-------------+\n",
      "|   Mini|        10500|\n",
      "+-------+-------------+\n",
      "\n",
      "Task1 time: 7.824317455291748 seconds\n",
      "-------------------------Start get_transform_sql_query-------------------------\n",
      "\n",
      "\n",
      "\u001b[92mINFO: \u001b[0mCreating temp view for the transform:\n",
      "df.createOrReplaceTempView(\u001b[33m\"\u001b[39;49;00m\u001b[33mspark_ai_temp_view_1100037772\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "-------------------------Current table schema from df is:-------------------------\n",
      "\n",
      " product, string\n",
      "category, string\n",
      "revenue, bigint\n",
      "\n",
      "-------------------------Current sample vals are:-------------------------\n",
      "\n",
      " (product, string, ['Normal', 'Normal', 'Mini'])\n",
      "(category, string, ['Cellphone', 'Tablet', 'Tablet'])\n",
      "(revenue, bigint, ['6000', '1500', '5500'])\n",
      "\n",
      "-------------------------Current table comment is-------------------------\n",
      "\n",
      " \n",
      "\n",
      "-------------------------Start generating sql query with a prompt with few-shot examples-------------------------\n",
      "\n",
      "\n",
      "-------------------------Input prompt is:-------------------------\n",
      "\n",
      " You are an assistant for writing professional Spark SQL queries. \n",
      "Given a question, you need to write a Spark SQL query to answer the question.\n",
      "The rules that you should follow for answering question:\n",
      "1.The answer only consists of Spark SQL query. No explaination. No \n",
      "2.SQL statements should be  Spark SQL query.\n",
      "3.ONLY use the verbatim column_name in your resulting SQL query; DO NOT include the type.\n",
      "4.Use the COUNT SQL function when the query asks for total number of some non-countable column.\n",
      "5.Use the SUM SQL function to accumulate the total number of countable column values.\n",
      "\n",
      "QUESTION: Given a Spark temp view `spark_ai_temp_view_14kjd0` with the following sample vals,\n",
      "    in the format (column_name, type, [sample_value_1, sample_value_2...]):\n",
      "```\n",
      "(a, string, [Kongur Tagh, Grossglockner])\n",
      "(b, int, [7649, 3798])\n",
      "(c, string, [China, Austria])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_14kjd0`: Find the mountain located in Japan.\n",
      "Answer:\n",
      "```SELECT `a` FROM `spark_ai_temp_view_14kjd0` WHERE `c` = 'Japan'```\n",
      "\n",
      "QUESTION: Given a Spark temp view `spark_ai_temp_view_12qcl3` with the following (columns, types, sample_values):\n",
      "```\n",
      "(Student, string, [student1, student2])\n",
      "(Birthday, string, [Dec 12 2005, 2006-03-04])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_12qcl3`: What is the total number of students with the birthday January 1, 2006?\n",
      "\n",
      "Answer:\n",
      "```SELECT COUNT(`Student`) FROM `spark_ai_temp_view_12qcl3` WHERE `Birthday` = 'January 1, 2006'```\n",
      "\n",
      "\n",
      "Question: Given a Spark temp view `spark_ai_temp_view_1100037772`  with the following sample vals,\n",
      " in the format (column_name, type, [sample_value_1, sample_value_2...]):\n",
      "```\n",
      "(product, string, ['Normal', 'Normal', 'Mini'])\n",
      "(category, string, ['Cellphone', 'Tablet', 'Tablet'])\n",
      "(revenue, bigint, ['6000', '1500', '5500'])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_1100037772`: Pivot the data by product and the revenue for each product\n",
      "Answer:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------The model replies:-------------------------\n",
      "\n",
      " ``` SELECT p.product, SUM(r.revenue) AS total_revenue FROM `spark_ai_temp_view_1100037772` r INNER JOIN (SELECT DISTINCT product FROM `spark_ai_temp_view_1100037772`) p ON r.product = p.product GROUP BY p.product ORDER BY total_revenue DESC NULLS LAST;``` \n",
      "\n",
      "-------------------------Spark retrieved sql:-------------------------\n",
      "\n",
      " SELECT p.product, SUM(r.revenue) AS total_revenue FROM `spark_ai_temp_view_1100037772` r INNER JOIN (SELECT DISTINCT product FROM `spark_ai_temp_view_1100037772`) p ON r.product = p.product GROUP BY p.product ORDER BY total_revenue DESC NULLS LAST;\n",
      "\n",
      "-------------------------End get_transform_sql_query-------------------------\n",
      "\n",
      " get_transform_sql_query_time: 9.542081594467163 seconds\n",
      "\n",
      "-------------------------Received query:-------------------------\n",
      "\n",
      " SELECT p.product, SUM(r.revenue) AS total_revenue FROM `spark_ai_temp_view_1100037772` r INNER JOIN (SELECT DISTINCT product FROM `spark_ai_temp_view_1100037772`) p ON r.product = p.product GROUP BY p.product ORDER BY total_revenue DESC NULLS LAST;\n",
      "\n",
      "+--------+-------------+\n",
      "| product|total_revenue|\n",
      "+--------+-------------+\n",
      "|    Mini|        10500|\n",
      "|Foldable|         9000|\n",
      "|  Normal|         7500|\n",
      "|     Pro|         7000|\n",
      "| Pro Max|         4500|\n",
      "+--------+-------------+\n",
      "\n",
      "Task2 time: 10.041518449783325 seconds\n",
      "-------------------------Start get_transform_sql_query-------------------------\n",
      "\n",
      "\n",
      "\u001b[92mINFO: \u001b[0mCreating temp view for the transform:\n",
      "df.createOrReplaceTempView(\u001b[33m\"\u001b[39;49;00m\u001b[33mspark_ai_temp_view_1100037772\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "-------------------------Current table schema from df is:-------------------------\n",
      "\n",
      " product, string\n",
      "category, string\n",
      "revenue, bigint\n",
      "\n",
      "-------------------------Current sample vals are:-------------------------\n",
      "\n",
      " (product, string, ['Normal', 'Normal', 'Mini'])\n",
      "(category, string, ['Cellphone', 'Tablet', 'Tablet'])\n",
      "(revenue, bigint, ['6000', '1500', '5500'])\n",
      "\n",
      "-------------------------Current table comment is-------------------------\n",
      "\n",
      " \n",
      "\n",
      "-------------------------Start generating sql query with a prompt with few-shot examples-------------------------\n",
      "\n",
      "\n",
      "-------------------------Input prompt is:-------------------------\n",
      "\n",
      " You are an assistant for writing professional Spark SQL queries. \n",
      "Given a question, you need to write a Spark SQL query to answer the question.\n",
      "The rules that you should follow for answering question:\n",
      "1.The answer only consists of Spark SQL query. No explaination. No \n",
      "2.SQL statements should be  Spark SQL query.\n",
      "3.ONLY use the verbatim column_name in your resulting SQL query; DO NOT include the type.\n",
      "4.Use the COUNT SQL function when the query asks for total number of some non-countable column.\n",
      "5.Use the SUM SQL function to accumulate the total number of countable column values.\n",
      "\n",
      "QUESTION: Given a Spark temp view `spark_ai_temp_view_14kjd0` with the following sample vals,\n",
      "    in the format (column_name, type, [sample_value_1, sample_value_2...]):\n",
      "```\n",
      "(a, string, [Kongur Tagh, Grossglockner])\n",
      "(b, int, [7649, 3798])\n",
      "(c, string, [China, Austria])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_14kjd0`: Find the mountain located in Japan.\n",
      "Answer:\n",
      "```SELECT `a` FROM `spark_ai_temp_view_14kjd0` WHERE `c` = 'Japan'```\n",
      "\n",
      "QUESTION: Given a Spark temp view `spark_ai_temp_view_12qcl3` with the following (columns, types, sample_values):\n",
      "```\n",
      "(Student, string, [student1, student2])\n",
      "(Birthday, string, [Dec 12 2005, 2006-03-04])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_12qcl3`: What is the total number of students with the birthday January 1, 2006?\n",
      "\n",
      "Answer:\n",
      "```SELECT COUNT(`Student`) FROM `spark_ai_temp_view_12qcl3` WHERE `Birthday` = 'January 1, 2006'```\n",
      "\n",
      "\n",
      "Question: Given a Spark temp view `spark_ai_temp_view_1100037772`  with the following sample vals,\n",
      " in the format (column_name, type, [sample_value_1, sample_value_2...]):\n",
      "```\n",
      "(product, string, ['Normal', 'Normal', 'Mini'])\n",
      "(category, string, ['Cellphone', 'Tablet', 'Tablet'])\n",
      "(revenue, bigint, ['6000', '1500', '5500'])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_1100037772`: Pivot the data by catagory and the revenue for each product\n",
      "Answer:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------The model replies:-------------------------\n",
      "\n",
      " ```SELECT c.category, p.product, SUM(r.revenue) AS total_revenue FROM spark_ai_temp_view_1100037772 r JOIN product p ON r.product = p.product JOIN category c ON p.category = c.name GROUP BY c.category, p.product;``` \n",
      "\n",
      "-------------------------Spark retrieved sql:-------------------------\n",
      "\n",
      " SELECT c.category, p.product, SUM(r.revenue) AS total_revenue FROM spark_ai_temp_view_1100037772 r JOIN product p ON r.product = p.product JOIN category c ON p.category = c.name GROUP BY c.category, p.product;\n",
      "\n",
      "\u001b[93mWARN: \u001b[0mGetting the following error: \n",
      "[TABLE_OR_VIEW_NOT_FOUND] The table or view `product` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 104;\n",
      "'Aggregate ['c.category, 'p.product], ['c.category, 'p.product, 'SUM('r.revenue) AS total_revenue#51]\n",
      "+- 'Join Inner, ('p.category = 'c.name)\n",
      "   :- 'Join Inner, ('r.product = 'p.product)\n",
      "   :  :- SubqueryAlias r\n",
      "   :  :  +- SubqueryAlias spark_ai_temp_view_1100037772\n",
      "   :  :     +- View (`spark_ai_temp_view_1100037772`, [product#0,category#1,revenue#2L])\n",
      "   :  :        +- LogicalRDD [product#0, category#1, revenue#2L], false\n",
      "   :  +- 'SubqueryAlias p\n",
      "   :     +- 'UnresolvedRelation [product], [], false\n",
      "   +- 'SubqueryAlias c\n",
      "      +- 'UnresolvedRelation [category], [], false\n",
      "\n",
      "\u001b[92mINFO: \u001b[0mRetrying with 3 retries left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------The model replies:-------------------------\n",
      "\n",
      "  SELECT category, product, revenue FROM spark_ai_temp_view_1100037772 GROUP BY category, product, revenue ORDER BY category, product; \n",
      "\n",
      "-------------------------Spark retrieved sql:-------------------------\n",
      "\n",
      "  SELECT category, product, revenue FROM spark_ai_temp_view_1100037772 GROUP BY category, product, revenue ORDER BY category, product;\n",
      "\n",
      "-------------------------End get_transform_sql_query-------------------------\n",
      "\n",
      " get_transform_sql_query_time: 11.873137950897217 seconds\n",
      "\n",
      "-------------------------Received query:-------------------------\n",
      "\n",
      "  SELECT category, product, revenue FROM spark_ai_temp_view_1100037772 GROUP BY category, product, revenue ORDER BY category, product;\n",
      "\n",
      "+---------+--------+-------+\n",
      "| category| product|revenue|\n",
      "+---------+--------+-------+\n",
      "|Cellphone|Foldable|   6500|\n",
      "|Cellphone|    Mini|   5000|\n",
      "|Cellphone|  Normal|   6000|\n",
      "|Cellphone|     Pro|   3000|\n",
      "|Cellphone| Pro Max|   4500|\n",
      "|   Tablet|Foldable|   2500|\n",
      "|   Tablet|    Mini|   5500|\n",
      "|   Tablet|  Normal|   1500|\n",
      "|   Tablet|     Pro|   4000|\n",
      "+---------+--------+-------+\n",
      "\n",
      "Task3 time: 12.121431112289429 seconds\n",
      "-------------------------Start get_transform_sql_query-------------------------\n",
      "\n",
      "\n",
      "\u001b[92mINFO: \u001b[0mCreating temp view for the transform:\n",
      "df.createOrReplaceTempView(\u001b[33m\"\u001b[39;49;00m\u001b[33mspark_ai_temp_view_1100037772\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "-------------------------Current table schema from df is:-------------------------\n",
      "\n",
      " product, string\n",
      "category, string\n",
      "revenue, bigint\n",
      "\n",
      "-------------------------Current sample vals are:-------------------------\n",
      "\n",
      " (product, string, ['Normal', 'Normal', 'Mini'])\n",
      "(category, string, ['Cellphone', 'Tablet', 'Tablet'])\n",
      "(revenue, bigint, ['6000', '1500', '5500'])\n",
      "\n",
      "-------------------------Current table comment is-------------------------\n",
      "\n",
      " \n",
      "\n",
      "-------------------------Start generating sql query with a prompt with few-shot examples-------------------------\n",
      "\n",
      "\n",
      "-------------------------Input prompt is:-------------------------\n",
      "\n",
      " You are an assistant for writing professional Spark SQL queries. \n",
      "Given a question, you need to write a Spark SQL query to answer the question.\n",
      "The rules that you should follow for answering question:\n",
      "1.The answer only consists of Spark SQL query. No explaination. No \n",
      "2.SQL statements should be  Spark SQL query.\n",
      "3.ONLY use the verbatim column_name in your resulting SQL query; DO NOT include the type.\n",
      "4.Use the COUNT SQL function when the query asks for total number of some non-countable column.\n",
      "5.Use the SUM SQL function to accumulate the total number of countable column values.\n",
      "\n",
      "QUESTION: Given a Spark temp view `spark_ai_temp_view_14kjd0` with the following sample vals,\n",
      "    in the format (column_name, type, [sample_value_1, sample_value_2...]):\n",
      "```\n",
      "(a, string, [Kongur Tagh, Grossglockner])\n",
      "(b, int, [7649, 3798])\n",
      "(c, string, [China, Austria])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_14kjd0`: Find the mountain located in Japan.\n",
      "Answer:\n",
      "```SELECT `a` FROM `spark_ai_temp_view_14kjd0` WHERE `c` = 'Japan'```\n",
      "\n",
      "QUESTION: Given a Spark temp view `spark_ai_temp_view_12qcl3` with the following (columns, types, sample_values):\n",
      "```\n",
      "(Student, string, [student1, student2])\n",
      "(Birthday, string, [Dec 12 2005, 2006-03-04])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_12qcl3`: What is the total number of students with the birthday January 1, 2006?\n",
      "\n",
      "Answer:\n",
      "```SELECT COUNT(`Student`) FROM `spark_ai_temp_view_12qcl3` WHERE `Birthday` = 'January 1, 2006'```\n",
      "\n",
      "\n",
      "Question: Given a Spark temp view `spark_ai_temp_view_1100037772`  with the following sample vals,\n",
      " in the format (column_name, type, [sample_value_1, sample_value_2...]):\n",
      "```\n",
      "(product, string, ['Normal', 'Normal', 'Mini'])\n",
      "(category, string, ['Cellphone', 'Tablet', 'Tablet'])\n",
      "(revenue, bigint, ['6000', '1500', '5500'])\n",
      "```\n",
      "Write a Spark SQL query to retrieve from view `spark_ai_temp_view_1100037772`: What are the best-selling and the second best-selling products in every category?\n",
      "Answer:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------The model replies:-------------------------\n",
      "\n",
      "  SELECT `c`.`category`, `p`.`product`, `r`.`revenue`, rank() OVER (PARTITION BY c.category ORDER BY r.revenue DESC) AS Product_Rank FROM `spark_ai_temp_view_1100037772` p JOIN (SELECT product, category FROM `spark_ai_temp_view_1100037772`) r ON p.product = r.product JOIN (SELECT DISTINCT category FROM `spark_ai_temp_view_1100037772`) c ON p.category = c.category WHERE Product_Rank <= 2; \n",
      "\n",
      "-------------------------Spark retrieved sql:-------------------------\n",
      "\n",
      "  SELECT `c`.`category`, `p`.`product`, `r`.`revenue`, rank() OVER (PARTITION BY c.category ORDER BY r.revenue DESC) AS Product_Rank FROM `spark_ai_temp_view_1100037772` p JOIN (SELECT product, category FROM `spark_ai_temp_view_1100037772`) r ON p.product = r.product JOIN (SELECT DISTINCT category FROM `spark_ai_temp_view_1100037772`) c ON p.category = c.category WHERE Product_Rank <= 2;\n",
      "\n",
      "\u001b[93mWARN: \u001b[0mGetting the following error: \n",
      "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Product_Rank` cannot be resolved. Did you mean one of the following? [`r`.`category`, `p`.`product`, `r`.`product`, `p`.`category`, `c`.`category`].; line 1 pos 371;\n",
      "'Project ['c.category, 'p.product, 'r.revenue, rank() windowspecdefinition('c.category, 'r.revenue DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS Product_Rank#71]\n",
      "+- 'Filter ('Product_Rank <= 2)\n",
      "   +- Join Inner, (category#1 = category#76)\n",
      "      :- Join Inner, (product#0 = product#72)\n",
      "      :  :- SubqueryAlias p\n",
      "      :  :  +- SubqueryAlias spark_ai_temp_view_1100037772\n",
      "      :  :     +- View (`spark_ai_temp_view_1100037772`, [product#0,category#1,revenue#2L])\n",
      "      :  :        +- LogicalRDD [product#0, category#1, revenue#2L], false\n",
      "      :  +- SubqueryAlias r\n",
      "      :     +- Project [product#72, category#73]\n",
      "      :        +- SubqueryAlias spark_ai_temp_view_1100037772\n",
      "      :           +- View (`spark_ai_temp_view_1100037772`, [product#72,category#73,revenue#74L])\n",
      "      :              +- LogicalRDD [product#72, category#73, revenue#74L], false\n",
      "      +- SubqueryAlias c\n",
      "         +- Distinct\n",
      "            +- Project [category#76]\n",
      "               +- SubqueryAlias spark_ai_temp_view_1100037772\n",
      "                  +- View (`spark_ai_temp_view_1100037772`, [product#75,category#76,revenue#77L])\n",
      "                     +- LogicalRDD [product#75, category#76, revenue#77L], false\n",
      "\n",
      "\u001b[92mINFO: \u001b[0mRetrying with 3 retries left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:21<00:00, 21.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------The model replies:-------------------------\n",
      "\n",
      "  SELECT c.category, p.product, rank() OVER (PARTITION BY c.category ORDER BY COUNT(p.product) DESC) AS rank FROM `spark_ai_temp_view_1100037772` AS tc JOIN (SELECT product, COUNT(*) AS COUNT FROM `spark_ai_temp_view_1100037772` GROUP BY product) AS pcnt ON tc.product = pcnt.product JOIN (SELECT category, COUNT(*) AS COUNT FROM `spark_ai_temp_view_1100037772` GROUP BY category) AS catcnt ON tc.category = catcnt.category JOIN (SELECT category, product, COUNT(*) AS count FROM `spark_ai_temp_view_1100037772` GROUP BY category, product) AS prodcat ON tc.category = prodcat.category AND tc.product = prodcat.product GROUP BY c.category, p.product ORDER BY c.category, rank; \n",
      "\n",
      "-------------------------Spark retrieved sql:-------------------------\n",
      "\n",
      "  SELECT c.category, p.product, rank() OVER (PARTITION BY c.category ORDER BY COUNT(p.product) DESC) AS rank FROM `spark_ai_temp_view_1100037772` AS tc JOIN (SELECT product, COUNT(*) AS COUNT FROM `spark_ai_temp_view_1100037772` GROUP BY product) AS pcnt ON tc.product = pcnt.product JOIN (SELECT category, COUNT(*) AS COUNT FROM `spark_ai_temp_view_1100037772` GROUP BY category) AS catcnt ON tc.category = catcnt.category JOIN (SELECT category, product, COUNT(*) AS count FROM `spark_ai_temp_view_1100037772` GROUP BY category, product) AS prodcat ON tc.category = prodcat.category AND tc.product = prodcat.product GROUP BY c.category, p.product ORDER BY c.category, rank;\n",
      "\n",
      "\u001b[93mWARN: \u001b[0mGetting the following error: \n",
      "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`category` cannot be resolved. Did you mean one of the following? [`tc`.`category`, `catcnt`.`category`, `prodcat`.`category`, `tc`.`revenue`, `pcnt`.`COUNT`].; line 1 pos 8;\n",
      "'Sort ['c.category ASC NULLS FIRST, 'rank ASC NULLS FIRST], true\n",
      "+- 'Aggregate ['c.category, 'p.product], ['c.category, 'p.product, rank() windowspecdefinition('c.category, 'COUNT('p.product) DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#83]\n",
      "   +- Join Inner, ((category#1 = category#91) AND (product#0 = product#90))\n",
      "      :- Join Inner, (category#1 = category#88)\n",
      "      :  :- Join Inner, (product#0 = product#84)\n",
      "      :  :  :- SubqueryAlias tc\n",
      "      :  :  :  +- SubqueryAlias spark_ai_temp_view_1100037772\n",
      "      :  :  :     +- View (`spark_ai_temp_view_1100037772`, [product#0,category#1,revenue#2L])\n",
      "      :  :  :        +- LogicalRDD [product#0, category#1, revenue#2L], false\n",
      "      :  :  +- SubqueryAlias pcnt\n",
      "      :  :     +- Aggregate [product#84], [product#84, count(1) AS COUNT#80L]\n",
      "      :  :        +- SubqueryAlias spark_ai_temp_view_1100037772\n",
      "      :  :           +- View (`spark_ai_temp_view_1100037772`, [product#84,category#85,revenue#86L])\n",
      "      :  :              +- LogicalRDD [product#84, category#85, revenue#86L], false\n",
      "      :  +- SubqueryAlias catcnt\n",
      "      :     +- Aggregate [category#88], [category#88, count(1) AS COUNT#81L]\n",
      "      :        +- SubqueryAlias spark_ai_temp_view_1100037772\n",
      "      :           +- View (`spark_ai_temp_view_1100037772`, [product#87,category#88,revenue#89L])\n",
      "      :              +- LogicalRDD [product#87, category#88, revenue#89L], false\n",
      "      +- SubqueryAlias prodcat\n",
      "         +- Aggregate [category#91, product#90], [category#91, product#90, count(1) AS count#82L]\n",
      "            +- SubqueryAlias spark_ai_temp_view_1100037772\n",
      "               +- View (`spark_ai_temp_view_1100037772`, [product#90,category#91,revenue#92L])\n",
      "                  +- LogicalRDD [product#90, category#91, revenue#92L], false\n",
      "\n",
      "\u001b[92mINFO: \u001b[0mRetrying with 2 retries left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------The model replies:-------------------------\n",
      "\n",
      "  SELECT c.category, p.product, rank() OVER (PARTITION BY c.category ORDER BY r.revenue DESC) AS rank FROM spark_ai_temp_view_1100037772 AS r JOIN product p ON p.product_id = r.product JOIN category c ON c.category_id = p.category_id GROUP BY c.category, p.product, r.revenue ORDER BY c.category, rank NULLS LAST; \n",
      "\n",
      "-------------------------Spark retrieved sql:-------------------------\n",
      "\n",
      "  SELECT c.category, p.product, rank() OVER (PARTITION BY c.category ORDER BY r.revenue DESC) AS rank FROM spark_ai_temp_view_1100037772 AS r JOIN product p ON p.product_id = r.product JOIN category c ON c.category_id = p.category_id GROUP BY c.category, p.product, r.revenue ORDER BY c.category, rank NULLS LAST;\n",
      "\n",
      "\u001b[93mWARN: \u001b[0mGetting the following error: \n",
      "[TABLE_OR_VIEW_NOT_FOUND] The table or view `product` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 146;\n",
      "'Sort ['c.category ASC NULLS FIRST, 'rank ASC NULLS LAST], true\n",
      "+- 'Aggregate ['c.category, 'p.product, 'r.revenue], ['c.category, 'p.product, rank() windowspecdefinition('c.category, 'r.revenue DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#98]\n",
      "   +- 'Join Inner, ('c.category_id = 'p.category_id)\n",
      "      :- 'Join Inner, ('p.product_id = 'r.product)\n",
      "      :  :- SubqueryAlias r\n",
      "      :  :  +- SubqueryAlias spark_ai_temp_view_1100037772\n",
      "      :  :     +- View (`spark_ai_temp_view_1100037772`, [product#0,category#1,revenue#2L])\n",
      "      :  :        +- LogicalRDD [product#0, category#1, revenue#2L], false\n",
      "      :  +- 'SubqueryAlias p\n",
      "      :     +- 'UnresolvedRelation [product], [], false\n",
      "      +- 'SubqueryAlias c\n",
      "         +- 'UnresolvedRelation [category], [], false\n",
      "\n",
      "\u001b[92mINFO: \u001b[0mRetrying with 1 retries left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------The model replies:-------------------------\n",
      "\n",
      "  SELECT c.category, p.product, rank() OVER (PARTITION BY c.category ORDER BY SUM(r.revenue) DESC) AS rank FROM `spark_ai_temp_view_1100037772` r JOIN `product p` ON r.product = p.product JOIN `category c` ON p.category = c.category GROUP BY c.category, p.product ORDER BY c.category, rank ASC LIMIT 2; \n",
      "\n",
      "-------------------------Spark retrieved sql:-------------------------\n",
      "\n",
      "  SELECT c.category, p.product, rank() OVER (PARTITION BY c.category ORDER BY SUM(r.revenue) DESC) AS rank FROM `spark_ai_temp_view_1100037772` r JOIN `product p` ON r.product = p.product JOIN `category c` ON p.category = c.category GROUP BY c.category, p.product ORDER BY c.category, rank ASC LIMIT 2;\n",
      "\n",
      "\u001b[93mWARN: \u001b[0mGetting the following error: \n",
      "[TABLE_OR_VIEW_NOT_FOUND] The table or view `product p` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 150;\n",
      "'GlobalLimit 2\n",
      "+- 'LocalLimit 2\n",
      "   +- 'Sort ['c.category ASC NULLS FIRST, 'rank ASC NULLS FIRST], true\n",
      "      +- 'Aggregate ['c.category, 'p.product], ['c.category, 'p.product, rank() windowspecdefinition('c.category, 'SUM('r.revenue) DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#101]\n",
      "         +- 'Join Inner, ('p.category = 'c.category)\n",
      "            :- 'Join Inner, ('r.product = 'p.product)\n",
      "            :  :- SubqueryAlias r\n",
      "            :  :  +- SubqueryAlias spark_ai_temp_view_1100037772\n",
      "            :  :     +- View (`spark_ai_temp_view_1100037772`, [product#0,category#1,revenue#2L])\n",
      "            :  :        +- LogicalRDD [product#0, category#1, revenue#2L], false\n",
      "            :  +- 'UnresolvedRelation [product p], [], false\n",
      "            +- 'UnresolvedRelation [category c], [], false\n",
      "\n",
      "\u001b[92mINFO: \u001b[0mNo more retries left, please modify the instruction or modify the generated code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------End get_transform_sql_query-------------------------\n",
      "\n",
      " get_transform_sql_query_time: 54.97767186164856 seconds\n",
      "\n",
      "-------------------------Received query:-------------------------\n",
      "\n",
      " \n",
      "\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 1, pos 0)\n\n== SQL ==\n\n^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask3 time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask3_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 66\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat are the best-selling and the second best-selling products in every category?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     67\u001b[0m task4_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#df.ai.transform(\"What is the difference between the revenue of each product and the revenue of the best-selling product in the same category of that product?\").show()\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#df.ai.plot()\u001b[39;00m\n",
      "File \u001b[0;32m~/zedong/pyspark-ai/pyspark_ai/ai_utils.py:42\u001b[0m, in \u001b[0;36mAIMethodWrapper.transform\u001b[0;34m(self, desc, cache)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, desc: \u001b[38;5;28mstr\u001b[39m, cache: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Transform the DataFrame using the given description.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m        The transformed DataFrame.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark_ai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zedong/pyspark-ai/pyspark_ai/pyspark_ai.py:608\u001b[0m, in \u001b[0;36mSparkAI.transform_df\u001b[0;34m(self, df, desc, cache)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-------------------------End get_transform_sql_query-------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m get_transform_sql_query_time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_transform_sql_query_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-------------------------Received query:-------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/zedong-vllm/lib/python3.10/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/zedong-vllm/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/zedong-vllm/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 1, pos 0)\n\n== SQL ==\n\n^^^\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain_community.llms import VLLM\n",
    "from pyspark_ai import SparkAI\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"32\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize the VLLM\n",
    "llm = VLLM(\n",
    "    #optional models:\n",
    "    #defog/sqlcoder-70b-alpha 93.0%\n",
    "    #defog/sqlcoder-7b-2      90.5%\n",
    "    #defog/sqlcoder-34b-alpha 84.0%\n",
    "    #defog/sqlcoder2          74.5%\n",
    "    #defog/sqlcoder-7b        71.0%\n",
    "    #defog/sqlcoder           64.6%\n",
    "    model=\"defog/sqlcoder-7b-2\",\n",
    "    trust_remote_code=True,\n",
    "    download_dir=\"/mnt/DP_disk2/models/Huggingface/\", #~/.conda/envs/zedong-vllm/lib/python3.10/site-packages/langchain_community/llms/vllm.py:88\n",
    ")\n",
    "\n",
    "# Initialize and activate SparkAI\n",
    "spark_ai = SparkAI(llm=llm,verbose=True)\n",
    "spark_ai.activate()\n",
    "\n",
    "# create a dataframe productRevenue\n",
    "df = spark_ai._spark.createDataFrame(\n",
    "    [\n",
    "        (\"Normal\", \"Cellphone\", 6000),\n",
    "        (\"Normal\", \"Tablet\", 1500),\n",
    "        (\"Mini\", \"Tablet\", 5500),\n",
    "        (\"Mini\", \"Cellphone\", 5000),\n",
    "        (\"Foldable\", \"Cellphone\", 6500),\n",
    "        (\"Foldable\", \"Tablet\", 2500),\n",
    "        (\"Pro\", \"Cellphone\", 3000),\n",
    "        (\"Pro\", \"Tablet\", 4000),\n",
    "        (\"Pro Max\", \"Cellphone\", 4500)\n",
    "    ],\n",
    "    [\"product\", \"category\", \"revenue\"]\n",
    ")\n",
    "\n",
    "init_time = time.time() - start_time\n",
    "print(f\"Init time: {init_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "df.ai.transform(\"What is the best-selling product?\").show()\n",
    "task1_time = time.time() - start_time\n",
    "print(f\"Task1 time: {task1_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "df.ai.transform(\"Pivot the data by product and the revenue for each product\").show()\n",
    "task2_time = time.time() - start_time\n",
    "print(f\"Task2 time: {task2_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "df.ai.transform(\"Pivot the data by catagory and the revenue for each product\").show()\n",
    "task3_time = time.time() - start_time\n",
    "print(f\"Task3 time: {task3_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "df.ai.transform(\"What are the best-selling and the second best-selling products in every category?\").show()\n",
    "task4_time = time.time() - start_time\n",
    "\n",
    "#df.ai.transform(\"What is the difference between the revenue of each product and the revenue of the best-selling product in the same category of that product?\").show()\n",
    "\n",
    "#df.ai.plot()\n",
    "\n",
    "print(f\"Init time: {init_time} seconds\")\n",
    "print(f\"Task1 time: {task1_time} seconds\")\n",
    "print(f\"Task2 time: {task2_time} seconds\")\n",
    "print(f\"Task3 time: {task3_time} seconds\")\n",
    "print(f\"Task4 time: {task4_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd0872-a34b-4b19-9848-f74a30a00ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
